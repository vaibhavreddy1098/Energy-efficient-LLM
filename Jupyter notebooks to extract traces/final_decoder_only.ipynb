{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f32bdb6-171b-4fad-91f3-aaad7a8a5a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Input ---\n",
      "The future of artificial intelligence is\n",
      "\n",
      "--- Output ---\n",
      "The future of artificial intelligence is uncertain.\n",
      "\n",
      "\"We're not sure what the future will look like,\" said Dr. Michael S. Schoenfeld, a professor of computer science at the University of California, Berkeley. \"But we're very\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 1. Load Pretrained Model and Tokenizer\n",
    "# We will use 'gpt2', a medium-sized decoder-only model.\n",
    "# The `GPT2LMHeadModel` is specifically designed for language modeling (text generation).\n",
    "# The first time you run this, it will download the model and tokenizer, which may take a few minutes.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model to evaluation mode. This is important for consistent and reproducible results.\n",
    "model.eval()\n",
    "\n",
    "# 2. Prepare the Input\n",
    "# This is the text prompt from which the model will start generating.\n",
    "input_prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "# Tokenize the input prompt. The tokenizer converts the text into a sequence of numbers (input IDs)\n",
    "# that the model can understand.\n",
    "# `return_tensors='pt'` ensures the output is a PyTorch tensor.\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors='pt')\n",
    "\n",
    "# 3. Perform Inference (Generate Output)\n",
    "# The `generate` method creates a sequence of tokens following the input prompt.\n",
    "# - `max_length`: The maximum length of the generated sequence (including the prompt).\n",
    "# - `num_return_sequences`: The number of different sequences to generate.\n",
    "# - `no_repeat_ngram_size`: Prevents the model from repeating the same n-grams.\n",
    "# - `pad_token_id`: Sets the padding token ID to the end-of-sequence token ID for open-ended generation.\n",
    "with torch.no_grad(): # Disable gradient calculation for inference to save memory and computations\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# 4. Decode the Output\n",
    "# The tokenizer is used again to convert the generated token IDs back into human-readable text.\n",
    "# `skip_special_tokens=True` removes any special tokens (like padding or end-of-sequence) from the output.\n",
    "generated_sequence = output_sequences[0].tolist()\n",
    "decoded_output = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "\n",
    "# 5. Display Input and Output\n",
    "print(\"--- Input ---\")\n",
    "print(input_prompt)\n",
    "print(\"\\n--- Output ---\")\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ec98cd-17cf-444b-853d-3358799fc68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained GPT-2 model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created a dummy input tensor with shape: torch.Size([1, 10])\n",
      "Tracing the model... (This may take a moment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1096: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trace file successfully created and saved to: traced_gpt2_model.pt\n",
      "\n",
      "Verifying the traced model by loading it and running inference...\n",
      "Verification successful!\n",
      "Output logits shape from traced model: torch.Size([1, 10, 50257])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 1. Load Pretrained Model and Tokenizer\n",
    "# It's crucial to load the model with `torchscript=True`.\n",
    "# This flag correctly handles the tied weights between the embedding and language model head,\n",
    "# which is necessary for successful tracing of models like GPT-2.\n",
    "print(\"Loading pretrained GPT-2 model and tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', torchscript=True)\n",
    "\n",
    "# Set the model to evaluation mode. This disables dropout and other training-specific layers.\n",
    "model.eval()\n",
    "\n",
    "# 2. Create a Dummy Input\n",
    "# The trace needs an example input to run through the model and record the operations.\n",
    "# The dimensions (batch_size, sequence_length) of this dummy input will be fixed in the trace.\n",
    "# Any future inputs to the traced model must have these exact dimensions.\n",
    "batch_size = 1\n",
    "sequence_length = 10  # You can choose a representative sequence length\n",
    "dummy_input = torch.randint(0, tokenizer.vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "print(f\"\\nCreated a dummy input tensor with shape: {dummy_input.shape}\")\n",
    "\n",
    "# 3. Trace the Model\n",
    "# We use `torch.jit.trace` to record a single forward pass of the model.\n",
    "# This creates a `ScriptModule` containing the traced computation graph.\n",
    "print(\"Tracing the model... (This may take a moment)\")\n",
    "with torch.no_grad():\n",
    "    traced_model = torch.jit.trace(model, dummy_input)\n",
    "\n",
    "# 4. Save the Traced Model\n",
    "# The traced model is saved to a file. This file can be loaded later\n",
    "# in Python or in other environments like a C++ application.\n",
    "trace_file_path = \"traced_gpt2_model.pt\"\n",
    "traced_model.save(trace_file_path)\n",
    "\n",
    "print(f\"\\nTrace file successfully created and saved to: {trace_file_path}\")\n",
    "\n",
    "# --- Optional: Verify the Traced Model ---\n",
    "print(\"\\nVerifying the traced model by loading it and running inference...\")\n",
    "\n",
    "# Load the saved trace file\n",
    "loaded_traced_model = torch.jit.load(trace_file_path)\n",
    "\n",
    "# Run the same dummy input through the loaded model\n",
    "with torch.no_grad():\n",
    "    outputs_from_traced_model = loaded_traced_model(dummy_input)\n",
    "    # The output of GPT2LMHeadModel is a tuple, where the first element is the logits\n",
    "    logits_from_traced_model = outputs_from_traced_model[0]\n",
    "\n",
    "print(\"Verification successful!\")\n",
    "print(f\"Output logits shape from traced model: {logits_from_traced_model.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6769728a-90f4-4d99-b88b-f57f886f4186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3 (monitor_gpu_stats):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\pynvml.py\", line 641, in _LoadNvmlLibrary\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "    nvmlLib = CDLL(os.path.join(os.getenv(\"ProgramFiles\", \"C:/Program Files\"), \"NVIDIA Corporation/NVSMI/nvml.dll\"))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\ctypes\\__init__.py\", line 379, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: Could not find module 'C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvml.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vaibh\\AppData\\Local\\Temp\\ipykernel_11788\\3555523487.py\", line 13, in monitor_gpu_stats\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\pynvml.py\", line 608, in nvmlInit\n",
      "    _LoadNvmlLibrary()\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\pynvml.py\", line 646, in _LoadNvmlLibrary\n",
      "    _nvmlCheckReturn(NVML_ERROR_LIBRARY_NOT_FOUND)\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\pynvml.py\", line 310, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_LibraryNotFound: NVML Shared Library Not Found\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\vaibh\\AppData\\Local\\Temp\\ipykernel_11788\\3555523487.py\", line 45, in monitor_gpu_stats\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\pynvml.py\", line 657, in nvmlShutdown\n",
      "    fn = _nvmlGetFunctionPointer(\"nvmlShutdown\")\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\pynvml.py\", line 325, in _nvmlGetFunctionPointer\n",
      "    raise NVMLError(NVML_ERROR_UNINITIALIZED)\n",
      "pynvml.NVMLError_Uninitialized: Uninitialized\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to GPU.\n",
      "\n",
      "Starting inference...\n",
      "Inference complete.\n",
      "\n",
      "--- Model Output ---\n",
      "The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI, we will need to consider the potential of AI to solve many of the problems we face today.\n",
      "\n",
      "The Future of Artificial Intelligence\n",
      "\n",
      "The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI, we will need to consider the potential of AI to solve many of the problems we face today.\n",
      "\n",
      "The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI, we will need to consider the potential of AI to solve many of the problems we face today.\n",
      "\n",
      "The future of artificial intelligence is a complex topic with many facets to consider\n",
      "\n",
      "--- Performance and Power Stats ---\n",
      "Inference Time: 4.95 seconds\n",
      "No NVIDIA GPU detected. Power stats are not available with this method.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import threading\n",
    "import time\n",
    "from pynvml import *\n",
    "\n",
    "# --- Monitoring Thread Function ---\n",
    "def monitor_gpu_stats(stop_event, stats):\n",
    "    \"\"\"\n",
    "    Monitors GPU stats in a separate thread and records them.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nvmlInit()\n",
    "        # Assuming you are using the first GPU (index 0)\n",
    "        handle = nvmlDeviceGetHandleByIndex(0)\n",
    "        \n",
    "        power_readings = []\n",
    "        temp_readings = []\n",
    "        util_readings = []\n",
    "\n",
    "        while not stop_event.is_set():\n",
    "            # Get power usage in Watts\n",
    "            power_usage = nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "            power_readings.append(power_usage)\n",
    "\n",
    "            # Get temperature in Celsius\n",
    "            temperature = nvmlDeviceGetTemperature(handle, NVML_TEMPERATURE_GPU)\n",
    "            temp_readings.append(temperature)\n",
    "\n",
    "            # Get utilization rates\n",
    "            utilization = nvmlDeviceGetUtilizationRates(handle)\n",
    "            util_readings.append(utilization.gpu)\n",
    "            \n",
    "            time.sleep(0.1) # Poll every 100ms\n",
    "            \n",
    "        # Store results\n",
    "        stats['power_avg_w'] = sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "        stats['power_peak_w'] = max(power_readings) if power_readings else 0\n",
    "        stats['temp_avg_c'] = sum(temp_readings) / len(temp_readings) if temp_readings else 0\n",
    "        stats['temp_peak_c'] = max(temp_readings) if temp_readings else 0\n",
    "        stats['util_avg_percent'] = sum(util_readings) / len(util_readings) if util_readings else 0\n",
    "        stats['util_peak_percent'] = max(util_readings) if util_readings else 0\n",
    "\n",
    "    finally:\n",
    "        nvmlShutdown()\n",
    "\n",
    "# --- Main Inference Code ---\n",
    "# 1. Load Model and Tokenizer\n",
    "print(\"Loading model...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "    print(\"Model moved to GPU.\")\n",
    "\n",
    "# 2. Prepare Input\n",
    "input_prompt = \"The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI,\"\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors='pt')\n",
    "if torch.cuda.is_available():\n",
    "    input_ids = input_ids.to('cuda')\n",
    "\n",
    "# 3. Start Monitoring and Perform Inference\n",
    "stats = {}\n",
    "stop_event = threading.Event()\n",
    "\n",
    "# Start the monitoring thread only if a CUDA device is found\n",
    "if torch.cuda.is_available():\n",
    "    monitor_thread = threading.Thread(target=monitor_gpu_stats, args=(stop_event, stats))\n",
    "    monitor_thread.start()\n",
    "\n",
    "print(\"\\nStarting inference...\")\n",
    "inference_start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=150,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "inference_duration = time.time() - inference_start_time\n",
    "print(\"Inference complete.\") \n",
    "\n",
    "# Stop the monitoring thread\n",
    "if torch.cuda.is_available():\n",
    "    stop_event.set()\n",
    "    monitor_thread.join()\n",
    "\n",
    "# 4. Decode and Display Results\n",
    "decoded_output = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Model Output ---\")\n",
    "print(decoded_output)\n",
    "\n",
    "print(f\"\\n--- Performance and Power Stats ---\")\n",
    "print(f\"Inference Time: {inference_duration:.2f} seconds\")\n",
    "if stats:\n",
    "    print(f\"Average GPU Power Draw: {stats['power_avg_w']:.2f} W\")\n",
    "    print(f\"Peak GPU Power Draw:    {stats['power_peak_w']:.2f} W\")\n",
    "    print(f\"Average GPU Temp:       {stats['temp_avg_c']:.1f}°C\")\n",
    "    print(f\"Peak GPU Temp:          {stats['temp_peak_c']:.1f}°C\")\n",
    "    print(f\"Average GPU Utilization:  {stats['util_avg_percent']:.1f}%\")\n",
    "    print(f\"Peak GPU Utilization:     {stats['util_peak_percent']:.1f}%\")\n",
    "else:\n",
    "    print(\"No NVIDIA GPU detected. Power stats are not available with this method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d0d07e-5b7a-4c1f-a743-36393b51fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting codecarbon\n",
      "  Downloading codecarbon-3.0.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: arrow in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from codecarbon) (1.3.0)\n",
      "Collecting click (from codecarbon)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting fief-client[cli] (from codecarbon)\n",
      "  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from codecarbon) (2.2.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from codecarbon) (0.22.0)\n",
      "Collecting psutil>=6.0.0 (from codecarbon)\n",
      "  Using cached psutil-7.0.0-cp37-abi3-win_amd64.whl.metadata (23 kB)\n",
      "Collecting py-cpuinfo (from codecarbon)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pydantic (from codecarbon)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: pynvml in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from codecarbon) (12.0.0)\n",
      "Collecting questionary (from codecarbon)\n",
      "  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting rapidfuzz (from codecarbon)\n",
      "  Downloading rapidfuzz-3.13.0-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from codecarbon) (2.32.3)\n",
      "Collecting rich (from codecarbon)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting typer (from codecarbon)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from arrow->codecarbon) (2.9.0.post0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from arrow->codecarbon) (2.9.0.20250516)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from click->codecarbon) (0.4.6)\n",
      "Collecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n",
      "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting yaspin (from fief-client[cli]->codecarbon)\n",
      "  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.16.0)\n",
      "Collecting cryptography>=3.4 (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon)\n",
      "  Downloading cryptography-45.0.4-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (4.12.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from cffi>=1.14->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas->codecarbon) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas->codecarbon) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas->codecarbon) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->codecarbon)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic->codecarbon)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->codecarbon)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pynvml->codecarbon) (12.575.51)\n",
      "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from questionary->codecarbon) (3.0.51)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->codecarbon) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->codecarbon) (2.3.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->codecarbon)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vaibh\\anaconda3\\envs\\pytorch\\lib\\site-packages (from rich->codecarbon) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->codecarbon)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer->codecarbon)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading codecarbon-3.0.2-py3-none-any.whl (610 kB)\n",
      "   ---------------------------------------- 0.0/610.1 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 262.1/610.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 610.1/610.1 kB 2.5 MB/s eta 0:00:00\n",
      "Using cached psutil-7.0.0-cp37-abi3-win_amd64.whl (244 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
      "Downloading cryptography-45.0.4-cp311-abi3-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.6/3.4 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.8/3.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.1/3.4 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.4/3.4 MB 4.9 MB/s eta 0:00:00\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 1.8/2.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 8.3 MB/s eta 0:00:00\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading questionary-2.1.0-py3-none-any.whl (36 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.3/1.6 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
      "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: py-cpuinfo, typing-inspection, termcolor, shellingham, rapidfuzz, pydantic-core, psutil, mdurl, click, annotated-types, yaspin, questionary, pydantic, markdown-it-py, httpx, cryptography, rich, jwcrypto, typer, fief-client, codecarbon\n",
      "\n",
      "   --- ------------------------------------  2/21 [termcolor]\n",
      "   ------- --------------------------------  4/21 [rapidfuzz]\n",
      "   ------- --------------------------------  4/21 [rapidfuzz]\n",
      "   --------- ------------------------------  5/21 [pydantic-core]\n",
      "  Attempting uninstall: psutil\n",
      "   --------- ------------------------------  5/21 [pydantic-core]\n",
      "    Found existing installation: psutil 5.9.0\n",
      "   --------- ------------------------------  5/21 [pydantic-core]\n",
      "    Uninstalling psutil-5.9.0:\n",
      "   --------- ------------------------------  5/21 [pydantic-core]\n",
      "      Successfully uninstalled psutil-5.9.0\n",
      "   --------- ------------------------------  5/21 [pydantic-core]\n",
      "   ----------- ----------------------------  6/21 [psutil]\n",
      "   ----------- ----------------------------  6/21 [psutil]\n",
      "   ----------- ----------------------------  6/21 [psutil]\n",
      "   ----------- ----------------------------  6/21 [psutil]\n",
      "   --------------- ------------------------  8/21 [click]\n",
      "   --------------- ------------------------  8/21 [click]\n",
      "   -------------------- ------------------- 11/21 [questionary]\n",
      "   ---------------------- ----------------- 12/21 [pydantic]\n",
      "   ---------------------- ----------------- 12/21 [pydantic]\n",
      "   ---------------------- ----------------- 12/21 [pydantic]\n",
      "   ---------------------- ----------------- 12/21 [pydantic]\n",
      "   ---------------------- ----------------- 12/21 [pydantic]\n",
      "   ---------------------- ----------------- 12/21 [pydantic]\n",
      "   ---------------------- ----------------- 12/21 [pydantic]\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "  Attempting uninstall: httpx\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "    Found existing installation: httpx 0.28.1\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "    Uninstalling httpx-0.28.1:\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "   -------------------------- ------------- 14/21 [httpx]\n",
      "   -------------------------- ------------- 14/21 [httpx]\n",
      "   ---------------------------- ----------- 15/21 [cryptography]\n",
      "   ---------------------------- ----------- 15/21 [cryptography]\n",
      "   ---------------------------- ----------- 15/21 [cryptography]\n",
      "   ---------------------------- ----------- 15/21 [cryptography]\n",
      "   ---------------------------- ----------- 15/21 [cryptography]\n",
      "   ---------------------------- ----------- 15/21 [cryptography]\n",
      "   ------------------------------ --------- 16/21 [rich]\n",
      "   ------------------------------ --------- 16/21 [rich]\n",
      "   ------------------------------ --------- 16/21 [rich]\n",
      "   ------------------------------ --------- 16/21 [rich]\n",
      "   ------------------------------ --------- 16/21 [rich]\n",
      "   ------------------------------ --------- 16/21 [rich]\n",
      "   ------------------------------ --------- 16/21 [rich]\n",
      "   -------------------------------- ------- 17/21 [jwcrypto]\n",
      "   ---------------------------------- ----- 18/21 [typer]\n",
      "   -------------------------------------- - 20/21 [codecarbon]\n",
      "   -------------------------------------- - 20/21 [codecarbon]\n",
      "   -------------------------------------- - 20/21 [codecarbon]\n",
      "   ---------------------------------------- 21/21 [codecarbon]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 click-8.2.1 codecarbon-3.0.2 cryptography-45.0.4 fief-client-0.20.0 httpx-0.27.2 jwcrypto-1.5.6 markdown-it-py-3.0.0 mdurl-0.1.2 psutil-7.0.0 py-cpuinfo-9.0.0 pydantic-2.11.7 pydantic-core-2.33.2 questionary-2.1.0 rapidfuzz-3.13.0 rich-14.0.0 shellingham-1.5.4 termcolor-2.3.0 typer-0.16.0 typing-inspection-0.4.1 yaspin-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6523f062-36fc-4369-b800-5f0b1619b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 15:04:30] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 15:04:30] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 15:04:30] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 15:04:33] We saw that you have a 12th Gen Intel(R) Core(TM) i7-12650H but we don't know it. Please contact us.\n",
      "[codecarbon WARNING @ 15:04:33] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 15:04:33] CPU Model on constant consumption mode: 12th Gen Intel(R) Core(TM) i7-12650H\n",
      "[codecarbon WARNING @ 15:04:33] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 15:04:33] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 15:04:33] No GPU found.\n",
      "[codecarbon INFO @ 15:04:33] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 15:04:33] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 15:04:33]   Platform system: Windows-11-10.0.26200-SP0\n",
      "[codecarbon INFO @ 15:04:33]   Python version: 3.12.9\n",
      "[codecarbon INFO @ 15:04:33]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 15:04:33]   Available RAM : 15.676 GB\n",
      "[codecarbon INFO @ 15:04:33]   CPU count: 16 thread(s) in 16 physical CPU(s)\n",
      "[codecarbon INFO @ 15:04:33]   CPU model: 12th Gen Intel(R) Core(TM) i7-12650H\n",
      "[codecarbon INFO @ 15:04:33]   GPU count: None\n",
      "[codecarbon INFO @ 15:04:33]   GPU model: None\n",
      "[codecarbon INFO @ 15:04:33] Emissions data (if any) will be saved to file C:\\Users\\vaibh\\Downloads\\pytorch\\emissions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cuda.\n",
      "\n",
      "Starting inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:04:40] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 15:04:40] Energy consumed for RAM : 0.000019 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 15:04:40] Delta energy consumed for CPU with constant : 0.000080 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 15:04:40] Energy consumed for All CPU : 0.000080 kWh\n",
      "[codecarbon INFO @ 15:04:40] 0.000098 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:04:40] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete.\n",
      "\n",
      "--- Model Output ---\n",
      "The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI, we will need to consider the potential of AI to solve many of the problems we face today.\n",
      "\n",
      "The Future of Artificial Intelligence\n",
      "\n",
      "The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI, we will need to consider the potential of AI to solve many of the problems we face today.\n",
      "\n",
      "The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI, we will need to consider the potential of AI to solve many of the problems we face today.\n",
      "\n",
      "The future of artificial intelligence is a complex topic with many facets to consider\n",
      "\n",
      "Measurement complete. Check the 'emissions.csv' file for power and carbon stats.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from codecarbon import track_emissions\n",
    "\n",
    "# The @track_emissions decorator will automatically measure the\n",
    "# energy and carbon footprint of this function.\n",
    "@track_emissions(project_name=\"GPT2_Inference\")\n",
    "def run_gpt2_inference():\n",
    "    \"\"\"\n",
    "    Loads the GPT-2 model and performs a single inference task.\n",
    "    \"\"\"\n",
    "    # 1. Load Model and Tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    print(f\"Model moved to {device}.\")\n",
    "\n",
    "    # 2. Prepare Input\n",
    "    input_prompt = \"The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI,\"\n",
    "    input_ids = tokenizer.encode(input_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    # 3. Perform Inference\n",
    "    print(\"\\nStarting inference...\")\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    print(\"Inference complete.\")\n",
    "\n",
    "    # 4. Decode Output\n",
    "    decoded_output = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n--- Model Output ---\")\n",
    "    print(decoded_output)\n",
    "\n",
    "\n",
    "# --- Run the tracked function ---\n",
    "if __name__ == \"__main__\":\n",
    "    run_gpt2_inference()\n",
    "    print(\"\\nMeasurement complete. Check the 'emissions.csv' file for power and carbon stats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69daab2-1e61-4f68-a3e6-b9114dc82071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Loading model and moving to GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17 (monitor_gpu_stats):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\threading.py\", line 1012, in run\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\vaibh\\AppData\\Local\\Temp\\ipykernel_11788\\4099947937.py\", line 57, in monitor_gpu_stats\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\pynvml.py\", line 657, in nvmlShutdown\n",
      "    fn = _nvmlGetFunctionPointer(\"nvmlShutdown\")\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vaibh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\pynvml.py\", line 325, in _nvmlGetFunctionPointer\n",
      "    raise NVMLError(NVML_ERROR_UNINITIALIZED)\n",
      "pynvml.NVMLError_Uninitialized: Uninitialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting inference on GPU...NVML Error in monitoring thread: NVML Shared Library Not Found\n",
      "\n",
      "Inference complete.\n",
      "\n",
      "--- Model Output ---\n",
      "The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI, we will need to consider the potential of AI to solve many of the problems we face today.\n",
      "\n",
      "The Future of Artificial Intelligence\n",
      "\n",
      "The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI, we will need to consider the potential of AI to solve many of the problems we face today.\n",
      "\n",
      "The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI, we will need to consider the potential of AI to solve many of the problems we face today.\n",
      "\n",
      "The future of artificial intelligence is a complex topic with many facets to consider\n",
      "\n",
      "--- GPU Performance and Power Stats ---\n",
      "Inference Time: 3.30 seconds\n",
      "Average GPU Power Draw: 0.00 W\n",
      "Peak GPU Power Draw:    0.00 W\n",
      "Average GPU Temp:       0.0°C\n",
      "Peak GPU Temp:          0.0°C\n",
      "Average GPU Utilization:  0.0%\n",
      "Peak GPU Utilization:     0.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import threading\n",
    "import time\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from pynvml import *\n",
    "except ImportError:\n",
    "    print(\"Error: 'pynvml' library not found. Please install it using 'pip install pynvml'\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Step 1: Verify CUDA Setup ---\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(\"PyTorch cannot find a CUDA-enabled GPU.\")\n",
    "    print(\"Please check the following:\")\n",
    "    print(\"1. You have an NVIDIA GPU.\")\n",
    "    print(\"2. The latest NVIDIA drivers are installed.\")\n",
    "    print(\"3. You have installed PyTorch with CUDA support. See https://pytorch.org/\")\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Monitoring Thread Function (Corrected) ---\n",
    "def monitor_gpu_stats(stop_event, stats_dict):\n",
    "    \"\"\"Monitors GPU stats in a separate thread and records them.\"\"\"\n",
    "    try:\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(0)\n",
    "        \n",
    "        power_readings = []\n",
    "        temp_readings = []\n",
    "        util_readings = []\n",
    "\n",
    "        while not stop_event.is_set():\n",
    "            power_readings.append(nvmlDeviceGetPowerUsage(handle) / 1000.0) # Watts\n",
    "            temp_readings.append(nvmlDeviceGetTemperature(handle, NVML_TEMPERATURE_GPU))\n",
    "            util_readings.append(nvmlDeviceGetUtilizationRates(handle).gpu)\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        # --- FIX STARTS HERE ---\n",
    "        # Safely calculate stats, handling the case where lists might be empty\n",
    "        # if the inference was too fast for the monitor to collect data.\n",
    "        stats_dict['power_avg_w'] = sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "        stats_dict['power_peak_w'] = max(power_readings) if power_readings else 0\n",
    "        stats_dict['temp_avg_c'] = sum(temp_readings) / len(temp_readings) if temp_readings else 0\n",
    "        stats_dict['temp_peak_c'] = max(temp_readings) if temp_readings else 0\n",
    "        stats_dict['util_avg_percent'] = sum(util_readings) / len(util_readings) if util_readings else 0\n",
    "        stats_dict['util_peak_percent'] = max(util_readings) if util_readings else 0\n",
    "        # --- FIX ENDS HERE ---\n",
    "\n",
    "    except NVMLError as error:\n",
    "        print(f\"NVML Error in monitoring thread: {error}\")\n",
    "    finally:\n",
    "        nvmlShutdown()\n",
    "\n",
    "# --- Main Inference Code ---\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"CUDA is available. Using device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"Loading model and moving to GPU...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "input_prompt = \"The future of artificial intelligence is a complex topic with many facets to consider. As we develop more advanced AI,\"\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "stats = {}\n",
    "stop_event = threading.Event()\n",
    "\n",
    "monitor_thread = threading.Thread(target=monitor_gpu_stats, args=(stop_event, stats))\n",
    "monitor_thread.start()\n",
    "\n",
    "print(\"\\nStarting inference on GPU...\")\n",
    "inference_start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=150,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "inference_duration = time.time() - inference_start_time\n",
    "print(\"Inference complete.\")\n",
    "\n",
    "stop_event.set()\n",
    "monitor_thread.join()\n",
    "\n",
    "decoded_output = tokenizer.decode(output_sequences[0].cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Model Output ---\")\n",
    "print(decoded_output)\n",
    "\n",
    "print(f\"\\n--- GPU Performance and Power Stats ---\")\n",
    "print(f\"Inference Time: {inference_duration:.2f} seconds\")\n",
    "\n",
    "# --- FIX STARTS HERE ---\n",
    "# Use the .get() method on the dictionary for safe access. This provides a\n",
    "# default value (0) if the key doesn't exist for any reason, preventing a KeyError.\n",
    "print(f\"Average GPU Power Draw: {stats.get('power_avg_w', 0):.2f} W\")\n",
    "print(f\"Peak GPU Power Draw:    {stats.get('power_peak_w', 0):.2f} W\")\n",
    "print(f\"Average GPU Temp:       {stats.get('temp_avg_c', 0):.1f}°C\")\n",
    "print(f\"Peak GPU Temp:          {stats.get('temp_peak_c', 0):.1f}°C\")\n",
    "print(f\"Average GPU Utilization:  {stats.get('util_avg_percent', 0):.1f}%\")\n",
    "print(f\"Peak GPU Utilization:     {stats.get('util_peak_percent', 0):.1f}%\")\n",
    "# --- FIX ENDS HERE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66c0c0-c099-47c4-8877-3ef0ffbf1491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
