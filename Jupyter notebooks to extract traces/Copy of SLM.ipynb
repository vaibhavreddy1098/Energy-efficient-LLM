{"cells":[{"cell_type":"markdown","source":["**Author:** Dr. Shahriar Hossain <br>\n","**Topic of the code:** ChatBot using a Simple/Small Language Model (SLM) <br>\n","**Video explaining this code:**  <br>\n","https://youtu.be/OaLemi9JIl8 <br>\n","**My YT Channel:** https://www.youtube.com/@C4A <br>\n","**Web:** https://computing4all.com/"],"metadata":{"id":"VPWhIOUxyrXq"},"id":"VPWhIOUxyrXq"},{"cell_type":"code","execution_count":null,"id":"b1bdeafd","metadata":{"id":"b1bdeafd"},"outputs":[],"source":["import random\n","import nltk\n","from nltk import bigrams, FreqDist, ConditionalFreqDist"]},{"cell_type":"code","execution_count":null,"id":"c065d0d2","metadata":{"scrolled":true,"id":"c065d0d2","outputId":"81485dbb-f045-49ca-be83-ea5a6c543daa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739535807219,"user_tz":-330,"elapsed":10,"user":{"displayName":"Vaibhav Reddy","userId":"11269168650758548799"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"id":"a0756e51","metadata":{"id":"a0756e51","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1739535811127,"user_tz":-330,"elapsed":513,"user":{"displayName":"Vaibhav Reddy","userId":"11269168650758548799"}},"outputId":"abf35034-b3c9-4b5e-c5f0-397567455a31"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'data'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-31368fbacb08>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtext_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data'"]}],"source":["import os\n","import string\n","\n","## Keep your training documents in a folder named 'data'\n","input_data_dir = \"data\"\n","\n","# String of punctuation without the full stop\n","punctuation = string.punctuation.replace('.', '')  # Retain the full stop\n","\n","def is_hidden(filepath):\n","    return os.path.basename(filepath).startswith('.')\n","\n","text_data=\"\"\n","for filename in os.listdir(input_data_dir):\n","    filepath = os.path.join(input_data_dir, filename)\n","    if not is_hidden(filepath):\n","        with open(filepath) as infile:\n","            for line in infile:\n","                if line.strip():  # Check if line is not just whitespace\n","                    # Remove all punctuation except full stops\n","                    for char in punctuation:\n","                        line = line.replace(char, '')\n","                    text_data += line"]},{"cell_type":"code","execution_count":1,"id":"95c2da5f","metadata":{"id":"95c2da5f","outputId":"48da0246-2670-41da-a406-e3de123caa64","executionInfo":{"status":"error","timestamp":1739537003997,"user_tz":-330,"elapsed":519,"user":{"displayName":"Vaibhav Reddy","userId":"11269168650758548799"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'text_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-54cce619489d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'text_data' is not defined"]}],"source":["len(text_data)"]},{"cell_type":"code","execution_count":null,"id":"4929aabb","metadata":{"id":"4929aabb"},"outputs":[],"source":["# Tokenize the text into words\n","# Lowercasing for consistency\n","words = nltk.word_tokenize(text_data.lower())\n","\n","# Generate bigrams\n","bi_grams = list(bigrams(words))\n","\n","# Calculate frequency distribution for each bigram\n","bi_gram_freq_dist = FreqDist(bi_grams)"]},{"cell_type":"code","execution_count":null,"id":"da52d781","metadata":{"scrolled":true,"id":"da52d781","outputId":"dc5fd732-0dac-41bf-ddc7-4081dbce3a16"},"outputs":[{"name":"stdout","output_type":"stream","text":["(('asian', 'exporters'), 1)\n","(('exporters', 'fear'), 2)\n","(('fear', 'damage'), 1)\n","(('damage', 'from'), 8)\n","(('from', 'u'), 27)\n"]}],"source":["from itertools import islice\n","# Print the first five elements of the dictionary\n","first_five_items = list(islice(bi_gram_freq_dist.items(), 5))\n","for item in first_five_items:\n","    print(item)"]},{"cell_type":"code","execution_count":null,"id":"1b4290ff","metadata":{"id":"1b4290ff"},"outputs":[],"source":["# Compute conditional frequency distribution of bigrams\n","bi_gram_freq = ConditionalFreqDist(bi_grams)"]},{"cell_type":"code","execution_count":null,"id":"2af0da7b","metadata":{"id":"2af0da7b","outputId":"0d1da5b7-33b0-45fd-aead-76435438a781"},"outputs":[{"data":{"text/plain":["FreqDist({'gas': 216, 'rubber': 39, 'resources': 9, 'for': 3, 'float': 3, 'disasters': 2, 'that': 2, 'lt': 2, 'lower': 1, 'beverages': 1, ...})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["bi_gram_freq['natural']"]},{"cell_type":"code","execution_count":null,"id":"8d611fb6","metadata":{"id":"8d611fb6"},"outputs":[],"source":["import heapq\n","\n","topk=3\n","# Create a dictionary to hold the top topk bigrams for each first word\n","top_bigrams_per_first_word = {}\n","\n","# Iterate over the bigram frequency distribution\n","for (first_word, second_word), freq in bi_gram_freq_dist.items():\n","    # Initialize an empty heap for the first_word if it doesn't exist\n","    if first_word not in top_bigrams_per_first_word:\n","        top_bigrams_per_first_word[first_word] = []\n","\n","    # Add to the heap and maintain top topk\n","    heapq.heappush(top_bigrams_per_first_word[first_word],\n","                   (freq, second_word))\n","    if len(top_bigrams_per_first_word[first_word]) > topk:\n","        heapq.heappop(top_bigrams_per_first_word[first_word])\n"]},{"cell_type":"code","execution_count":null,"id":"9847e087","metadata":{"id":"9847e087","outputId":"b7530b14-b102-42f6-ac5f-d030410c8ecc"},"outputs":[{"data":{"text/plain":["[(9, 'resources'), (216, 'gas'), (39, 'rubber')]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["top_bigrams_per_first_word['natural']"]},{"cell_type":"code","execution_count":null,"id":"7031ed37","metadata":{"id":"7031ed37"},"outputs":[],"source":["# Convert the heap to a simple list for each first word\n","for first_word in top_bigrams_per_first_word:\n","    sorted_bigrams = sorted(\n","        top_bigrams_per_first_word[first_word], reverse=True)\n","    top_bigrams_list = []\n","    for freq, second_word in sorted_bigrams:\n","        top_bigrams_list.append(second_word)\n","    top_bigrams_per_first_word[first_word] = top_bigrams_list\n","\n","# Use these filtered bigrams to create a ConditionalFreqDist\n","filtered_bi_grams = []\n","for first_word in top_bigrams_per_first_word:\n","    for second_word in top_bigrams_per_first_word[first_word]:\n","        filtered_bi_grams.append((first_word, second_word))\n","\n","bi_gram_freq = ConditionalFreqDist(filtered_bi_grams)"]},{"cell_type":"code","execution_count":null,"id":"8d187620","metadata":{"id":"8d187620"},"outputs":[],"source":["def generate_sentence(word, num_words):\n","    word =word.lower()\n","    for _ in range(num_words):\n","        print(word, end=' ')\n","        next_words = [item for item, freq in bi_gram_freq[word].items()]\n","        if len(next_words) > 0:\n","            # Randomly choose a next word\n","            word = random.choice(next_words)\n","        else:\n","            break  # Break if the word has no following words\n","    print()"]},{"cell_type":"code","execution_count":null,"id":"cd6d1728","metadata":{"id":"cd6d1728","outputId":"2e5d333d-0114-4644-d735-9b9fa7986042","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"error","timestamp":1739535770755,"user_tz":-330,"elapsed":634,"user":{"displayName":"Vaibhav Reddy","userId":"11269168650758548799"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'generate_sentence' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d248d62110b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'generate_sentence' is not defined"]}],"source":["generate_sentence('the', 100)"]},{"cell_type":"code","execution_count":null,"id":"11c0d450","metadata":{"id":"11c0d450"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[{"file_id":"1j1PlqH9cAlFRiJECDxjtA0ouGsBVZzPS","timestamp":1739536012975}]}},"nbformat":4,"nbformat_minor":5}