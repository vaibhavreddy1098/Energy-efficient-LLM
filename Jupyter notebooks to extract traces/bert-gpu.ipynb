{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf47af0-c626-4687-90a3-466d2dd56ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing ---\n",
      "BERT model loaded with 12 layers.\n",
      "Model is on device: cuda\n",
      "CUDA is available: True\n",
      "Current CUDA device name: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "\n",
      "--- Input Tensor Information ---\n",
      "Input IDs shape: torch.Size([2, 256])\n",
      "Attention Mask shape: torch.Size([2, 256])\n",
      "Token Type IDs shape: torch.Size([2, 256])\n",
      "\n",
      "--- Starting Profiling ---\n",
      "Saving trace file to: C:\\Users\\vaibh\\Downloads\\pytorch\\bert_trace_logs_with_gpu\n",
      "\n",
      "--- Profiling Complete ---\n",
      "Trace files saved.\n",
      "\n",
      "--- Example Model Output Shape (last_hidden_state) ---\n",
      "Last hidden state shape: torch.Size([2, 256, 768])\n",
      "Pooled output shape: torch.Size([2, 768])\n",
      "\n",
      "Remember to open the trace file in chrome://tracing or Perfetto.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import os\n",
    "\n",
    "print(\"--- Initializing ---\")\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode (important for consistent behavior and disabling dropout)\n",
    "model.eval()\n",
    "\n",
    "# Check for CUDA availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"BERT model loaded with {model.config.num_hidden_layers} layers.\")\n",
    "print(f\"Model is on device: {device}\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Example text\n",
    "text = \"Hello, how are you doing today? This is an example sentence for BERT.\"\n",
    "# To increase GPU activity for a better trace, let's make the input longer and create a small batch\n",
    "long_text = \" \".join([text] * 10) # Repeat the sentence to make it longer\n",
    "batch_texts = [long_text, \"Another slightly different sentence for the batch.\"] # Create a batch of 2 sentences\n",
    "\n",
    "# Tokenize and prepare input tensors\n",
    "# Using padding=True (default pads to longest in batch) or 'max_length' for consistent size\n",
    "inputs = tokenizer(batch_texts, return_tensors='pt', padding='max_length', truncation=True, max_length=256) # Increased max_length to 256\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "token_type_ids = inputs['token_type_ids'].to(device) if 'token_type_ids' in inputs else None\n",
    "\n",
    "print(f\"\\n--- Input Tensor Information ---\")\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention Mask shape: {attention_mask.shape}\")\n",
    "if token_type_ids is not None:\n",
    "    print(f\"Token Type IDs shape: {token_type_ids.shape}\")\n",
    "else:\n",
    "    print(\"Token Type IDs are not present (common for single segment tasks).\")\n",
    "\n",
    "# Define the directory to save the trace file\n",
    "log_dir = \"bert_trace_logs_with_gpu\" # Changed log dir name to avoid conflicts\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n--- Starting Profiling ---\")\n",
    "print(f\"Saving trace file to: {os.path.abspath(log_dir)}\")\n",
    "\n",
    "# Use the profiler context manager\n",
    "# activities: Specify what to profile (CPU and CUDA activities)\n",
    "# record_shapes: Records input shapes of operators\n",
    "# profile_memory: Records memory usage\n",
    "# with_stack: Records stack information\n",
    "# schedule: Defines when to start/stop profiling and how to save.\n",
    "#          Here, `on_trace_ready` is used to save the trace to a file.\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=2, active=5, repeat=1), # Increased active steps\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(log_dir),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    # Run more iterations to ensure ample data for the trace\n",
    "    for i in range(8): # Increased loop range\n",
    "        # Use a dummy input if you want to skip a step, or use the actual input\n",
    "        # to ensure the model runs for every prof.step()\n",
    "        with record_function(f\"BERT_inference_step_{i}\"): # Add step number to recorded function\n",
    "            output = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        prof.step()\n",
    "\n",
    "print(\"\\n--- Profiling Complete ---\")\n",
    "print(\"Trace files saved.\")\n",
    "\n",
    "# You can access the model output here if you need to inspect it\n",
    "# The output typically includes `last_hidden_state` and `pooler_output`\n",
    "print(f\"\\n--- Example Model Output Shape (last_hidden_state) ---\")\n",
    "if hasattr(output, 'last_hidden_state'):\n",
    "    print(f\"Last hidden state shape: {output.last_hidden_state.shape}\")\n",
    "if hasattr(output, 'pooler_output'):\n",
    "    print(f\"Pooled output shape: {output.pooler_output.shape}\")\n",
    "\n",
    "print(\"\\nRemember to open the trace file in chrome://tracing or Perfetto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979d562-f5df-4594-adf5-2eef4ae220e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
