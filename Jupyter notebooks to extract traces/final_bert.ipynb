{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "447b450d-928b-439b-80a3-9792c64d555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "773246af-d79f-4c45-bfdc-858aa5b18606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model loaded with 12 layers.\n",
      "Model is on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode (important for consistent behavior and disabling dropout)\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"BERT model loaded with {model.config.num_hidden_layers} layers.\")\n",
    "print(f\"Model is on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153a0ab6-8918-40a3-bfe1-f3e13eb936bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 128])\n",
      "Attention Mask shape: torch.Size([1, 128])\n",
      "Token Type IDs shape: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Hello, how are you doing today? This is an example sentence for BERT.\"\n",
    "\n",
    "# Tokenize and prepare input tensors\n",
    "# The `return_tensors='pt'` argument ensures PyTorch tensors are returned.\n",
    "# `padding='max_length'` and `truncation=True` are important for consistent input size.\n",
    "inputs = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "token_type_ids = inputs['token_type_ids'].to(device) if 'token_type_ids' in inputs else None\n",
    "\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention Mask shape: {attention_mask.shape}\")\n",
    "if token_type_ids is not None:\n",
    "    print(f\"Token Type IDs shape: {token_type_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a7e79b-af35-4cb9-9974-24f49845adc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trace file to: C:\\Users\\vaibh\\Downloads\\pytorch\\bert_trace_logs\n",
      "Profiling complete. Trace files saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the trace file\n",
    "log_dir = \"bert_trace_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving trace file to: {os.path.abspath(log_dir)}\")\n",
    "\n",
    "# Use the profiler context manager\n",
    "# activities: Specify what to profile (CPU and CUDA activities)\n",
    "# record_shapes: Records input shapes of operators\n",
    "# profile_memory: Records memory usage\n",
    "# with_stack: Records stack information\n",
    "# schedule: Defines when to start/stop profiling and how to save.\n",
    "#          Here, `on_trace_ready` is used to save the trace to a file.\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(log_dir),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    for i in range(5):  # Run a few iterations to capture a meaningful trace\n",
    "        with record_function(\"BERT_inference\"):\n",
    "            # Pass inputs to the model.\n",
    "            # The BertModel returns a BaseModelOutputWithPoolingAndCrossAttentions object.\n",
    "            # The hidden_states for all layers can be accessed via `output.hidden_states` if `output_hidden_states=True` was passed to from_pretrained.\n",
    "            # For `BertModel`, by default it only returns the last hidden state and pooled output.\n",
    "            # To get all hidden states, you'd usually use `BertConfig` and set `output_hidden_states=True`.\n",
    "            # However, for basic tracing, just running the forward pass is sufficient.\n",
    "            output = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        prof.step()\n",
    "\n",
    "print(\"Profiling complete. Trace files saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16af322-c42c-4852-b7c3-b1a4b6e08dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
