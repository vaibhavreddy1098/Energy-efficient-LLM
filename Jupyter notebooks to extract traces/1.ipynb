{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e627c216-67cb-473f-8f17-fe037489c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::linear        19.31%      60.800us       100.00%     314.900us     314.900us      38.000us         4.17%     911.000us     911.000us           0 b           0 b     128.00 Kb           0 b             1  \n",
      "         aten::addmm        59.26%     186.600us        59.26%     186.600us     186.600us     797.000us        87.49%     797.000us     797.000us           0 b           0 b     128.00 Kb     128.00 Kb             1  \n",
      "             aten::t        12.35%      38.900us        21.44%      67.500us      67.500us      40.000us         4.39%      76.000us      76.000us           0 b           0 b           0 b           0 b             1  \n",
      "     aten::transpose         7.49%      23.600us         9.08%      28.600us      28.600us      22.000us         2.41%      36.000us      36.000us           0 b           0 b           0 b           0 b             1  \n",
      "    aten::as_strided         1.59%       5.000us         1.59%       5.000us       5.000us      14.000us         1.54%      14.000us      14.000us           0 b           0 b           0 b           0 b             1  \n",
      "            [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b    -897.00 Kb    -897.00 Kb             4  \n",
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 314.900us\n",
      "Self CUDA time total: 911.000us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaibh\\AppData\\Local\\Temp\\ipykernel_1824\\3548757733.py:22: FutureWarning: The attribute `use_cuda` will be deprecated soon, please use ``use_device = 'cuda'`` instead.\n",
      "  with profiler.profile(with_stack=True, profile_memory=True, use_cuda=True) as prof:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# Define a simple encoder-only model\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(512, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Initialize model and input\n",
    "model = EncoderModel().cuda()\n",
    "input_tensor = torch.rand(128, 512).cuda()\n",
    "\n",
    "# Synchronize CUDA before profiling\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Profile the forward pass with CUDA enabled\n",
    "with profiler.profile(with_stack=True, profile_memory=True, use_cuda=True) as prof:\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# Synchronize CUDA after profiling\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Print profiling results, sorting by GPU execution time\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0e8469-1b48-437f-ae01-736f8929a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::linear        13.33%      37.000us       100.00%     277.600us     277.600us      12.000us         2.11%     568.000us     568.000us           0 b           0 b     128.00 Kb           0 b             1  \n",
      "         aten::addmm        61.67%     171.200us        61.67%     171.200us     171.200us     539.000us        94.89%     539.000us     539.000us           0 b           0 b     128.00 Kb     128.00 Kb             1  \n",
      "             aten::t        14.05%      39.000us        25.00%      69.400us      69.400us       7.000us         1.23%      17.000us      17.000us           0 b           0 b           0 b           0 b             1  \n",
      "     aten::transpose         8.86%      24.600us        10.95%      30.400us      30.400us       7.000us         1.23%      10.000us      10.000us           0 b           0 b           0 b           0 b             1  \n",
      "    aten::as_strided         2.09%       5.800us         2.09%       5.800us       5.800us       3.000us         0.53%       3.000us       3.000us           0 b           0 b           0 b           0 b             1  \n",
      "            [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b    -897.00 Kb    -897.00 Kb             4  \n",
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 277.600us\n",
      "Self CUDA time total: 568.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.profiler\n",
    "\n",
    "# Define a simple encoder-only model\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(512, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Initialize model and input\n",
    "model = EncoderModel().cuda()\n",
    "input_tensor = torch.rand(128, 512).cuda()\n",
    "\n",
    "# Synchronize CUDA before profiling to ensure accurate timing\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Use torch.profiler to properly track GPU and CPU activities\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# Synchronize CUDA again after profiling\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Print profiling results, sorted by GPU execution time\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca61c1a-958e-479d-95ce-8b105bdc1b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.1186\n",
      "Epoch [20/100], Loss: 0.0929\n",
      "Epoch [30/100], Loss: 0.0847\n",
      "Epoch [40/100], Loss: 0.0818\n",
      "Epoch [50/100], Loss: 0.0802\n",
      "Epoch [60/100], Loss: 0.0789\n",
      "Epoch [70/100], Loss: 0.0774\n",
      "Epoch [80/100], Loss: 0.0757\n",
      "Epoch [90/100], Loss: 0.0737\n",
      "Epoch [100/100], Loss: 0.0708\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 1\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Generate dummy data\n",
    "X_train = torch.rand(100, input_size)  # 100 samples, each with 10 features\n",
    "y_train = torch.rand(100, output_size)  # Target values\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    outputs = model(X_train)  # Forward pass\n",
    "    loss = criterion(outputs, y_train)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd08403-6b20-4e26-88e8-a29911c986e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\").cuda()\n",
    "model.eval()\n",
    "\n",
    "# Prepare input\n",
    "inputs = tokenizer(\"Profiling GPT-2 with PyTorch profiler\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Profile and export trace\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2),\n",
    "    on_trace_ready=None,  # We'll export manually\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    for _ in range(4):  # match wait+warmup+active\n",
    "        with torch.no_grad():\n",
    "            model(**inputs)\n",
    "        prof.step()\n",
    "\n",
    "# Export to Chrome trace format\n",
    "prof.export_chrome_trace(\"gpt2_trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fbf73-1ba2-4a54-ab37-864a3e5fa1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
