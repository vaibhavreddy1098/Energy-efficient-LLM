{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd5e29e-6fb4-450f-acf5-43d8fef79de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,458,916 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# For demonstration, let's create a dummy vocabulary and data\n",
    "# In a real scenario, you'd load actual vocabulary and data.\n",
    "SRC_VOCAB_SIZE = 100\n",
    "TRG_VOCAB_SIZE = 100\n",
    "MAX_LEN = 10\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src len, batch size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src len, batch size, emb dim]\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # For a unidirectional LSTM, outputs will contain hidden states for each time step.\n",
    "        # hidden and cell are the final hidden and cell states of the last layer.\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size] (this is typically a single token at a time)\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # cell = [n layers, batch size, hid dim]\n",
    "\n",
    "        # Expand input to [1, batch size] as RNNs expect sequence length as first dim\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, emb dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [1, batch size, hid dim]\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # cell = [n layers, batch size, hid dim]\n",
    "\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [src len, batch size]\n",
    "        # trg = [trg len, batch size]\n",
    "        # teacher_forcing_ratio is probability to use actual target output as next input\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Encoder hidden and cell state\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # First input to the decoder is the <sos> token (start of sequence)\n",
    "        # Assuming <sos> token is 0 for simplicity. In real data, it's typically a specific index.\n",
    "        input = trg[0,:] # Or a dedicated <sos> token for inference\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # Pass input token, encoder hidden and cell state to decoder\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # Store prediction in outputs tensor\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Decide if we're going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # Get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # If teacher forcing, use actual next token as input, else use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Model Hyperparameters\n",
    "INPUT_DIM = SRC_VOCAB_SIZE\n",
    "OUTPUT_DIM = TRG_VOCAB_SIZE\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate models\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "# A simple way to count parameters (useful for initial profiling)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# Dummy data for demonstration\n",
    "def generate_dummy_batch(src_vocab_size, trg_vocab_size, max_len, batch_size):\n",
    "    src_len = random.randint(5, max_len)\n",
    "    trg_len = random.randint(5, max_len)\n",
    "    src = torch.randint(1, src_vocab_size, (src_len, batch_size)).to(DEVICE) # 0 is usually <pad>\n",
    "    trg = torch.randint(1, trg_vocab_size, (trg_len, batch_size)).to(DEVICE)\n",
    "    return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070ed537-5a24-406e-baf7-df2ae068eb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training with PyTorch Profiler ---\n",
      "Epoch 1/5\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.05%     204.097us         0.05%     204.097us      68.032us     276.630ms        40.14%     276.630ms      92.210ms           0 b           0 b           0 b           0 b             3  \n",
      "                                     Train_Step_Batch_2        18.51%      75.789ms        25.92%     106.118ms     106.118ms      73.333ms        10.64%     106.151ms     106.151ms           0 b     -87.50 Kb           0 b      27.89 Mb             1  \n",
      "                                     Train_Step_Batch_3        11.36%      46.522ms        20.19%      82.661ms      82.661ms      43.965ms         6.38%      82.703ms      82.703ms           0 b     -75.00 Kb           0 b      30.65 Mb             1  \n",
      "                                     Train_Step_Batch_4        13.50%      55.296ms        20.01%      81.953ms      81.953ms      53.484ms         7.76%      81.972ms      81.972ms           0 b     -62.50 Kb           0 b      30.11 Mb             1  \n",
      "autograd::engine::evaluate_function: CudnnRnnBackwar...         1.21%       4.943ms        18.52%      75.840ms       4.213ms       2.684ms         0.39%      76.489ms       4.249ms           0 b           0 b      90.73 Mb    -208.90 Mb            18  \n",
      "                                      CudnnRnnBackward0         0.53%       2.186ms        16.65%      68.167ms       3.787ms       2.094ms         0.30%      68.496ms       3.805ms           0 b           0 b     257.56 Mb    -768.00 Kb            18  \n",
      "                              aten::_cudnn_rnn_backward        12.92%      52.889ms        16.11%      65.982ms       3.666ms      44.983ms         6.53%      66.402ms       3.689ms           0 b           0 b     258.31 Mb    -591.42 Mb            18  \n",
      "                               Optimizer.step#Adam.step         3.13%      12.832ms         8.54%      34.950ms      11.650ms       6.526ms         0.95%      36.279ms      12.093ms           0 b         -12 b           0 b     -85.45 Mb             3  \n",
      "                                             aten::lstm         1.27%       5.187ms         5.27%      21.566ms       1.198ms       4.848ms         0.70%      22.548ms       1.253ms           0 b           0 b      34.59 Mb           0 b            18  \n",
      "                                       aten::_cudnn_rnn         3.52%      14.416ms         3.72%      15.244ms     846.878us      14.094ms         2.04%      16.554ms     919.667us           0 b           0 b      34.59 Mb    -295.91 Mb            18  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 409.483ms\n",
      "Self CUDA time total: 689.219ms\n",
      "\n",
      "  Epoch Loss: 4.6049\n",
      "Epoch 2/5\n",
      "  Epoch Loss: 4.6068\n",
      "Epoch 3/5\n",
      "  Epoch Loss: 4.6048\n",
      "Epoch 4/5\n",
      "  Epoch Loss: 4.6023\n",
      "Epoch 5/5\n",
      "  Epoch Loss: 4.6055\n",
      "\n",
      "--- Profiling Complete ---\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# --- Training Setup (simplified) ---\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # Assuming 0 is <pad> token\n",
    "\n",
    "def train_step(model, src, trg, optimizer, criterion, clip):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, trg)\n",
    "\n",
    "    # trg = [trg len, batch size]\n",
    "    # output = [trg len, batch size, output dim]\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "\n",
    "    output = output[1:].view(-1, output_dim) # Flatten for NLLLoss\n",
    "    trg = trg[1:].view(-1)                   # Flatten target\n",
    "\n",
    "    loss = criterion(output, trg)\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # Gradient clipping\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "CLIP = 1.0 # Gradient clipping value\n",
    "\n",
    "# --- Holistic Trace Analysis Integration ---\n",
    "\n",
    "# Define a profiling schedule\n",
    "# wait: Number of steps to wait before starting to record\n",
    "# warmup: Number of steps to skip for warmup (e.g., CUDA initialization)\n",
    "# active: Number of steps to record\n",
    "# repeat: Number of times to repeat the cycle\n",
    "schedule = torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1)\n",
    "\n",
    "# List of activities to profile (CPU, CUDA, etc.)\n",
    "activities = [ProfilerActivity.CPU]\n",
    "if torch.cuda.is_available():\n",
    "    activities.append(ProfilerActivity.CUDA)\n",
    "\n",
    "# Function to be called after each profiling cycle.\n",
    "# This is where you can save results, send to TensorBoard, etc.\n",
    "def trace_handler(p):\n",
    "    print(p.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "    # You can save the trace in different formats:\n",
    "    # p.export_chrome_trace(\"trace.json\")\n",
    "    # p.export_stacks(\"stack_trace.txt\")\n",
    "    # To use TensorBoard:\n",
    "    # p.export_chrome_trace(f\"/tmp/trace_{p.step_num}.json\")\n",
    "    # p.export_chrome_trace(f\"/path/to/logs/my_run/trace_{p.step_num}.json\")\n",
    "    # In a real scenario, you'd use a SummaryWriter to log to TensorBoard.\n",
    "    # from torch.utils.tensorboard import SummaryWriter\n",
    "    # writer = SummaryWriter(\"/path/to/logs/my_run\")\n",
    "    # writer.add_trace(p.export_chrome_trace()) # This is conceptual, use p.events().prof_result.save() for actual traces\n",
    "\n",
    "\n",
    "print(\"\\n--- Starting Training with PyTorch Profiler ---\")\n",
    "\n",
    "with profile(\n",
    "    schedule=schedule,\n",
    "    activities=activities,\n",
    "    on_trace_ready=trace_handler, # Call trace_handler when a cycle is complete\n",
    "    with_stack=True,              # Capture stack information\n",
    "    profile_memory=True,          # Profile memory usage\n",
    "    record_shapes=True            # Record input shapes to ops\n",
    ") as prof:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Simulate a few training batches\n",
    "        for i in range(10): # Let's do 10 batches per epoch for profiling\n",
    "            src, trg = generate_dummy_batch(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "            # Wrap the training step with record_function for clearer labeling in the trace\n",
    "            with record_function(f\"Train_Step_Batch_{i}\"):\n",
    "                loss = train_step(model, src, trg, optimizer, criterion, CLIP)\n",
    "                epoch_loss += loss\n",
    "\n",
    "            # Step the profiler after each training step\n",
    "            prof.step()\n",
    "\n",
    "        print(f\"  Epoch Loss: {epoch_loss / 10:.4f}\") # Average loss over 10 batches\n",
    "\n",
    "print(\"\\n--- Profiling Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307bfda-14a4-4ee0-aa48-1384b4164c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
