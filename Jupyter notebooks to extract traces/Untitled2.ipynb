{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0407d59-c2d8-4884-b674-e0fd3ccb1d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model: t5-small...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     71\u001b[39m     example_text = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33m    Artificial intelligence (AI) is intelligence demonstrated by machines,\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[33m    as opposed to the natural intelligence displayed by humans and animals.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33m    the human mind, such as \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlearning\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mproblem-solving\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mrun_encoder_decoder_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     84\u001b[39m     example_text_2 = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[33m    The quick brown fox jumps over the lazy dog. This sentence is famous for\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[33m    containing all letters of the English alphabet. It is often used for\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[33m    testing typewriters and computer keyboards, as well as in typography,\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[33m    and for demonstrating text-based fonts and other applications.\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mrun_encoder_decoder_inference\u001b[39m\u001b[34m(input_text, model_name)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03mLoads a pre-trained encoder-decoder model, performs inference,\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03mand prints the input and generated output.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[33;03m                      (e.g., \"t5-small\", \"t5-base\").\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading tokenizer and model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m tokenizer = \u001b[43mT5Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[32m     16\u001b[39m model = T5ForConditionalGeneration.from_pretrained(model_name)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1885\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1871\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1868\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   1870\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1871\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def run_encoder_decoder_inference(input_text: str, model_name: str = \"t5-small\"):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained encoder-decoder model, performs inference,\n",
    "    and prints the input and generated output.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The text to be processed by the model.\n",
    "        model_name (str): The name of the pre-trained T5 model to use\n",
    "                          (e.g., \"t5-small\", \"t5-base\").\n",
    "    \"\"\"\n",
    "    print(f\"Loading tokenizer and model: {model_name}...\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    # --- Step 1: Prepare Input ---\n",
    "    # T5 models often expect a task prefix, e.g., \"summarize: \" for summarization\n",
    "    task_prefix = \"summarize: \"\n",
    "    full_input = task_prefix + input_text\n",
    "\n",
    "    print(f\"\\n--- Input Text ---\")\n",
    "    print(full_input)\n",
    "\n",
    "    # Encode the input text\n",
    "    # return_tensors='pt' ensures PyTorch tensors are returned\n",
    "    input_ids = tokenizer(full_input, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # --- Step 2: Generate Output ---\n",
    "    print(f\"\\n--- Generating Output (Max length: 150, Min length: 40) ---\")\n",
    "    # The 'generate' method handles the encoder-decoder attention mechanisms\n",
    "    # and beam search for quality output.\n",
    "    # You can adjust parameters like max_length, min_length, num_beams, etc.\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=150,  # Maximum length of the generated summary\n",
    "        min_length=40,   # Minimum length of the generated summary\n",
    "        num_beams=4,     # Number of beams for beam search (higher means better quality but slower)\n",
    "        early_stopping=True # Stop when all hypotheses have reached the end of the sentence\n",
    "    )\n",
    "\n",
    "    # --- Step 3: Decode Output ---\n",
    "    # Decode the generated IDs back to human-readable text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Generated Output ---\")\n",
    "    print(generated_text)\n",
    "\n",
    "    # --- Optional: Illustrating a simple trace (for deployment, not for \"extraction\" during inference) ---\n",
    "    # If you wanted to trace the *model* for deployment (e.g., to ONNX or TorchScript),\n",
    "    # you would typically do it separately, after you are satisfied with the model\n",
    "    # and before deploying it. This is not \"extracting\" it during runtime inference.\n",
    "    # Here's how you might trace a simple forward pass for PyTorch's TorchScript:\n",
    "    print(\"\\n--- Illustrating Model Tracing (Conceptual for Deployment) ---\")\n",
    "    try:\n",
    "        # Create a dummy input for tracing\n",
    "        dummy_input = tokenizer(\"Hello world!\", return_tensors=\"pt\").input_ids\n",
    "        traced_model = torch.jit.trace(model, dummy_input)\n",
    "        print(f\"Model successfully traced with TorchScript. Type: {type(traced_model)}\")\n",
    "        # In a real scenario, you would save this traced_model:\n",
    "        # traced_model.save(\"traced_t5_model.pt\")\n",
    "        print(\"Note: The traced model is for deployment/optimization, not directly part of this live inference output.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not trace model (this is normal if not specifically configured for tracing all ops): {e}\")\n",
    "        print(\"Tracing typically requires a more controlled environment or specific model configurations.\")\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    example_text = \"\"\"\n",
    "    Artificial intelligence (AI) is intelligence demonstrated by machines,\n",
    "    as opposed to the natural intelligence displayed by humans and animals.\n",
    "    Leading AI textbooks define the field as the study of \"intelligent agents\":\n",
    "    any device that perceives its environment and takes actions that maximize\n",
    "    its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\"\n",
    "    is often used to describe machines that mimic \"cognitive\" functions that humans associate with\n",
    "    the human mind, such as \"learning\" and \"problem-solving\".\n",
    "    \"\"\"\n",
    "    run_encoder_decoder_inference(example_text)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    example_text_2 = \"\"\"\n",
    "    The quick brown fox jumps over the lazy dog. This sentence is famous for\n",
    "    containing all letters of the English alphabet. It is often used for\n",
    "    testing typewriters and computer keyboards, as well as in typography,\n",
    "    and for demonstrating text-based fonts and other applications.\n",
    "    \"\"\"\n",
    "    run_encoder_decoder_inference(example_text_2, model_name=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5af73c-2c70-4dc7-9d2f-2cbf6ca221f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
